{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH3XYIChaQZR",
    "outputId": "e1bd5346-5517-4b36-c59b-3e6752faa590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311 461 7800\n",
      "223.3270512820513 1707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_file = 'train.json'\n",
    "with open(train_file, 'r') as f:\n",
    "        df=pd.read_json(f)\n",
    "        \n",
    "df=df.T\n",
    "features=df[\"features\"].values\n",
    "valence = df[\"valence\"].values\n",
    "activation = df[\"activation\"].values\n",
    "\n",
    "all_lengths = [len(seq) for seq in features]\n",
    "max_length = max(all_lengths)\n",
    "x1 = [x for x in all_lengths if x > 300 and x <= 500]\n",
    "x2 = [x for x in all_lengths if x > 500]\n",
    "x3 = [x for x in all_lengths if x > 00]\n",
    "print(len(x1), len(x2),len(x3))\n",
    "\n",
    "average_length = np.mean(all_lengths)\n",
    "print(average_length,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Y_Lwatltbi7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_data(train_file):\n",
    "    with open(train_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "    df=df.T\n",
    "    valence = df[\"valence\"].values\n",
    "    activation = df[\"activation\"].values\n",
    "    features = df[\"features\"].values\n",
    "    lengths = [len(seq) for seq in features]\n",
    "    desired_seq_length = 500\n",
    "    padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "    # loop through each sequence in x\n",
    "    for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "         if len(seq) > desired_seq_length:\n",
    "            padded_x[i] = seq[:desired_seq_length]\n",
    "         else:\n",
    "            padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "    train_data_X = torch.from_numpy(padded_x).float()\n",
    "\n",
    "    # Define the label dictionary\n",
    "    label_dict = {(1, 1): 'joy', (1, 0): 'pleasure', (0, 0): 'sadness', (0, 1): 'anger'}\n",
    "    # Define a function to map valence and activation to label\n",
    "    def map_label(row):\n",
    "        return label_dict[(row['valence'], row['activation'])]\n",
    "    # Add a new column with the label for each row\n",
    "    df['label'] = df.apply(map_label, axis=1)\n",
    "    # Drop the valence and activation columns\n",
    "    df.drop(['valence', 'activation'], axis=1, inplace=True)\n",
    "    # create a dictionary to map the emotion labels to numbers\n",
    "    emotion_dict = {'joy': 0, 'pleasure': 1, 'sadness': 2, 'anger': 3}\n",
    "    # replace the emotion labels with numbers in the dataframe\n",
    "    df['label'] = df['label'].replace(emotion_dict)\n",
    "\n",
    "    labels = df[\"label\"].values\n",
    "    train_data_y = torch.from_numpy(labels)\n",
    "\n",
    "    return train_data_X,  train_data_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dev_file = 'dev.json'\n",
    "with open(dev_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "df=df.T\n",
    "features = df[\"features\"].values\n",
    "lengths = [len(seq) for seq in features]\n",
    "desired_seq_length = 500\n",
    "padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "# loop through each sequence in x\n",
    "for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "      if len(seq) > desired_seq_length:\n",
    "          padded_x[i] = seq[:desired_seq_length]\n",
    "      else:\n",
    "          padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "dev_data_X = torch.from_numpy(padded_x).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CGq5nKPvi-Jv"
   },
   "outputs": [],
   "source": [
    "# relatively complicated version with three convs and two lstms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCnnModel, self).__init__()\n",
    "         # Defining a 2D convolution layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "            # Defining another 2D convolution layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(60*64, 32)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)  # print input shape\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        #print(\"end of CNN\", x.shape)  # print after avgpool3 layer\n",
    "        x=x.squeeze(-1)\n",
    "        x=x.transpose(1,2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        #print(x.shape) \n",
    "        x,_ = self.lstm2(x) \n",
    "        #print(\"after lstm\", x.shape) # print after LSTM layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print(x.shape) # print after LSTM layer\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)  # print after classifier\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)  # print after activation\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyCnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QT6yNODf5hJ6",
    "outputId": "f84674c8-859f-4571-f0d2-5bf84ad45c59"
   },
   "outputs": [],
   "source": [
    "train_file = 'train.json'\n",
    "\n",
    "train_data_X,  train_data_y = load_data(train_file)\n",
    "\n",
    "\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "train_data = Data.TensorDataset(train_data_X, train_data_y)   # merge the features x data and label y into one dataset\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = Data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "# print(len(train_loader)) = 244\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCnnModel()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KckTKgJZdN_y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train_model(model, train_loader, train_rate, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "  \n",
    "    batch_num =len(train_loader)\n",
    "    train_batch_num = round(batch_num* train_rate)\n",
    "\n",
    "    train_loss_all = []\n",
    "    train_acc_all = []\n",
    "    val_loss_all = []\n",
    "    val_acc_all = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "         print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "         print(\"-\"*10)\n",
    "        \n",
    "       # there are two stages for each epoch\n",
    "         train_loss = 0.0\n",
    "         train_corrects = 0\n",
    "         train_num =0\n",
    "         val_loss = 0.0\n",
    "         val_corrects = 0.0\n",
    "         val_num = 0\n",
    "       \n",
    "         for step, (b_x, b_y) in enumerate(train_loader):\n",
    "              b_x = b_x.to(device)\n",
    "              b_y = b_y.to(device)\n",
    "\n",
    "              if  step < train_batch_num:\n",
    "                  \n",
    "                model.train()\n",
    "\n",
    "                output= model(b_x)\n",
    "                pre_lab = torch.argmax(output, dim=1)\n",
    "                loss = criterion(output, b_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * b_x.size(0)\n",
    "                train_corrects += torch.sum(pre_lab == b_y.data)\n",
    "                train_num += b_x.size(0)\n",
    "                \n",
    "              else:\n",
    "                \n",
    "                model.eval()\n",
    "                output= model(b_x)\n",
    "                pre_lab = torch.argmax(output, dim=1)\n",
    "                loss = criterion(output, b_y)\n",
    "                val_loss += loss.item() * b_x.size(0)\n",
    "                \n",
    "                val_corrects += torch.sum(pre_lab == b_y.data)\n",
    "                val_num += b_x.size(0)\n",
    "                \n",
    "          \n",
    "         train_loss_all.append(train_loss/train_num)\n",
    "         train_acc_all.append(train_corrects.double().item()/train_num)\n",
    "         val_loss_all.append(val_loss/val_num)\n",
    "         val_acc_all.append(val_corrects.double().item()/val_num)\n",
    "\n",
    "         print('{} Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
    "         print('{} Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
    "\n",
    "        \n",
    "        # store the best model with paremeters         # Keep track of the best model weights\n",
    "         if val_acc_all[-1]>best_acc:\n",
    "                       best_acc = val_acc_all[-1]\n",
    "                       best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1tsePVa0dyI",
    "outputId": "ea3c9676-a1b6-425d-bddb-50b5de0cb94c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/120\n",
      "----------\n",
      "0 Train Loss: 1.1707, Train Acc: 0.4503\n",
      "0 Val Loss: 1.3029, Val Acc: 0.3129\n",
      "Epoch 1/120\n",
      "----------\n",
      "1 Train Loss: 1.1266, Train Acc: 0.4758\n",
      "1 Val Loss: 1.0731, Val Acc: 0.5145\n",
      "Epoch 2/120\n",
      "----------\n",
      "2 Train Loss: 1.0988, Train Acc: 0.4899\n",
      "2 Val Loss: 1.2336, Val Acc: 0.3588\n",
      "Epoch 3/120\n",
      "----------\n",
      "3 Train Loss: 1.0838, Train Acc: 0.5051\n",
      "3 Val Loss: 1.1572, Val Acc: 0.4685\n",
      "Epoch 4/120\n",
      "----------\n",
      "4 Train Loss: 1.0821, Train Acc: 0.5095\n",
      "4 Val Loss: 1.1394, Val Acc: 0.4932\n",
      "Epoch 5/120\n",
      "----------\n",
      "5 Train Loss: 1.0655, Train Acc: 0.5168\n",
      "5 Val Loss: 1.1844, Val Acc: 0.4787\n",
      "Epoch 6/120\n",
      "----------\n",
      "6 Train Loss: 1.0594, Train Acc: 0.5228\n",
      "6 Val Loss: 1.3855, Val Acc: 0.3844\n",
      "Epoch 7/120\n",
      "----------\n",
      "7 Train Loss: 1.0451, Train Acc: 0.5346\n",
      "7 Val Loss: 1.0743, Val Acc: 0.4864\n",
      "Epoch 8/120\n",
      "----------\n",
      "8 Train Loss: 1.0426, Train Acc: 0.5386\n",
      "8 Val Loss: 1.0708, Val Acc: 0.5272\n",
      "Epoch 9/120\n",
      "----------\n",
      "9 Train Loss: 1.0269, Train Acc: 0.5362\n",
      "9 Val Loss: 1.2363, Val Acc: 0.4838\n",
      "Epoch 10/120\n",
      "----------\n",
      "10 Train Loss: 1.0123, Train Acc: 0.5543\n",
      "10 Val Loss: 1.1268, Val Acc: 0.4804\n",
      "Epoch 11/120\n",
      "----------\n",
      "11 Train Loss: 1.0047, Train Acc: 0.5577\n",
      "11 Val Loss: 1.0201, Val Acc: 0.5400\n",
      "Epoch 12/120\n",
      "----------\n",
      "12 Train Loss: 0.9813, Train Acc: 0.5610\n",
      "12 Val Loss: 1.3733, Val Acc: 0.4022\n",
      "Epoch 13/120\n",
      "----------\n",
      "13 Train Loss: 0.9618, Train Acc: 0.5767\n",
      "13 Val Loss: 0.9837, Val Acc: 0.5417\n",
      "Epoch 14/120\n",
      "----------\n",
      "14 Train Loss: 0.9373, Train Acc: 0.5919\n",
      "14 Val Loss: 1.1545, Val Acc: 0.4787\n",
      "Epoch 15/120\n",
      "----------\n",
      "15 Train Loss: 0.9097, Train Acc: 0.6164\n",
      "15 Val Loss: 0.9775, Val Acc: 0.5825\n",
      "Epoch 16/120\n",
      "----------\n",
      "16 Train Loss: 0.8616, Train Acc: 0.6321\n",
      "16 Val Loss: 0.9715, Val Acc: 0.5527\n",
      "Epoch 17/120\n",
      "----------\n",
      "17 Train Loss: 0.8385, Train Acc: 0.6353\n",
      "17 Val Loss: 1.0559, Val Acc: 0.5196\n",
      "Epoch 18/120\n",
      "----------\n",
      "18 Train Loss: 0.7949, Train Acc: 0.6566\n",
      "18 Val Loss: 0.8375, Val Acc: 0.6259\n",
      "Epoch 19/120\n",
      "----------\n",
      "19 Train Loss: 0.7484, Train Acc: 0.6858\n",
      "19 Val Loss: 0.8436, Val Acc: 0.6310\n",
      "Epoch 20/120\n",
      "----------\n",
      "20 Train Loss: 0.7064, Train Acc: 0.7047\n",
      "20 Val Loss: 0.9993, Val Acc: 0.5723\n",
      "Epoch 21/120\n",
      "----------\n",
      "21 Train Loss: 0.6637, Train Acc: 0.7361\n",
      "21 Val Loss: 0.8627, Val Acc: 0.6497\n",
      "Epoch 22/120\n",
      "----------\n",
      "22 Train Loss: 0.6126, Train Acc: 0.7560\n",
      "22 Val Loss: 0.6984, Val Acc: 0.7194\n",
      "Epoch 23/120\n",
      "----------\n",
      "23 Train Loss: 0.5743, Train Acc: 0.7692\n",
      "23 Val Loss: 0.6667, Val Acc: 0.7245\n",
      "Epoch 24/120\n",
      "----------\n",
      "24 Train Loss: 0.5185, Train Acc: 0.8003\n",
      "24 Val Loss: 0.5284, Val Acc: 0.7925\n",
      "Epoch 25/120\n",
      "----------\n",
      "25 Train Loss: 0.4725, Train Acc: 0.8169\n",
      "25 Val Loss: 0.6616, Val Acc: 0.7279\n",
      "Epoch 26/120\n",
      "----------\n",
      "26 Train Loss: 0.4284, Train Acc: 0.8377\n",
      "26 Val Loss: 0.5678, Val Acc: 0.7653\n",
      "Epoch 27/120\n",
      "----------\n",
      "27 Train Loss: 0.3911, Train Acc: 0.8486\n",
      "27 Val Loss: 0.4175, Val Acc: 0.8393\n",
      "Epoch 28/120\n",
      "----------\n",
      "28 Train Loss: 0.3577, Train Acc: 0.8617\n",
      "28 Val Loss: 0.5279, Val Acc: 0.7849\n",
      "Epoch 29/120\n",
      "----------\n",
      "29 Train Loss: 0.3200, Train Acc: 0.8833\n",
      "29 Val Loss: 0.3614, Val Acc: 0.8469\n",
      "Epoch 30/120\n",
      "----------\n",
      "30 Train Loss: 0.3037, Train Acc: 0.8865\n",
      "30 Val Loss: 0.4430, Val Acc: 0.8180\n",
      "Epoch 31/120\n",
      "----------\n",
      "31 Train Loss: 0.2706, Train Acc: 0.9020\n",
      "31 Val Loss: 0.4998, Val Acc: 0.8044\n",
      "Epoch 32/120\n",
      "----------\n",
      "32 Train Loss: 0.2410, Train Acc: 0.9130\n",
      "32 Val Loss: 0.4380, Val Acc: 0.8240\n",
      "Epoch 33/120\n",
      "----------\n",
      "33 Train Loss: 0.2206, Train Acc: 0.9238\n",
      "33 Val Loss: 1.2870, Val Acc: 0.6641\n",
      "Epoch 34/120\n",
      "----------\n",
      "34 Train Loss: 0.1901, Train Acc: 0.9337\n",
      "34 Val Loss: 0.2071, Val Acc: 0.9260\n",
      "Epoch 35/120\n",
      "----------\n",
      "35 Train Loss: 0.1703, Train Acc: 0.9472\n",
      "35 Val Loss: 0.1997, Val Acc: 0.9337\n",
      "Epoch 36/120\n",
      "----------\n",
      "36 Train Loss: 0.1643, Train Acc: 0.9467\n",
      "36 Val Loss: 0.1663, Val Acc: 0.9396\n",
      "Epoch 37/120\n",
      "----------\n",
      "37 Train Loss: 0.1471, Train Acc: 0.9529\n",
      "37 Val Loss: 0.2322, Val Acc: 0.9158\n",
      "Epoch 38/120\n",
      "----------\n",
      "38 Train Loss: 0.1214, Train Acc: 0.9638\n",
      "38 Val Loss: 0.1468, Val Acc: 0.9566\n",
      "Epoch 39/120\n",
      "----------\n",
      "39 Train Loss: 0.1429, Train Acc: 0.9512\n",
      "39 Val Loss: 1.3418, Val Acc: 0.6794\n",
      "Epoch 40/120\n",
      "----------\n",
      "40 Train Loss: 0.1075, Train Acc: 0.9672\n",
      "40 Val Loss: 0.3943, Val Acc: 0.8546\n",
      "Epoch 41/120\n",
      "----------\n",
      "41 Train Loss: 0.1081, Train Acc: 0.9689\n",
      "41 Val Loss: 0.1649, Val Acc: 0.9439\n",
      "Epoch 42/120\n",
      "----------\n",
      "42 Train Loss: 0.0902, Train Acc: 0.9740\n",
      "42 Val Loss: 0.2032, Val Acc: 0.9260\n",
      "Epoch 43/120\n",
      "----------\n",
      "43 Train Loss: 0.0973, Train Acc: 0.9697\n",
      "43 Val Loss: 0.1870, Val Acc: 0.9235\n",
      "Epoch 44/120\n",
      "----------\n",
      "44 Train Loss: 0.0917, Train Acc: 0.9730\n",
      "44 Val Loss: 0.3062, Val Acc: 0.8988\n",
      "Epoch 45/120\n",
      "----------\n",
      "45 Train Loss: 0.0878, Train Acc: 0.9737\n",
      "45 Val Loss: 0.3079, Val Acc: 0.8971\n",
      "Epoch 46/120\n",
      "----------\n",
      "46 Train Loss: 0.0871, Train Acc: 0.9737\n",
      "46 Val Loss: 0.1063, Val Acc: 0.9626\n",
      "Epoch 47/120\n",
      "----------\n",
      "47 Train Loss: 0.0705, Train Acc: 0.9795\n",
      "47 Val Loss: 0.2551, Val Acc: 0.9065\n",
      "Epoch 48/120\n",
      "----------\n",
      "48 Train Loss: 0.0791, Train Acc: 0.9763\n",
      "48 Val Loss: 0.3371, Val Acc: 0.8861\n",
      "Epoch 49/120\n",
      "----------\n",
      "49 Train Loss: 0.0617, Train Acc: 0.9813\n",
      "49 Val Loss: 0.2099, Val Acc: 0.9218\n",
      "Epoch 50/120\n",
      "----------\n",
      "50 Train Loss: 0.0599, Train Acc: 0.9828\n",
      "50 Val Loss: 3.0003, Val Acc: 0.5604\n",
      "Epoch 51/120\n",
      "----------\n",
      "51 Train Loss: 0.0792, Train Acc: 0.9758\n",
      "51 Val Loss: 0.1809, Val Acc: 0.9447\n",
      "Epoch 52/120\n",
      "----------\n",
      "52 Train Loss: 0.0601, Train Acc: 0.9819\n",
      "52 Val Loss: 0.2115, Val Acc: 0.9328\n",
      "Epoch 53/120\n",
      "----------\n",
      "53 Train Loss: 0.0495, Train Acc: 0.9876\n",
      "53 Val Loss: 0.0411, Val Acc: 0.9881\n",
      "Epoch 54/120\n",
      "----------\n",
      "54 Train Loss: 0.0503, Train Acc: 0.9872\n",
      "54 Val Loss: 0.1444, Val Acc: 0.9422\n",
      "Epoch 55/120\n",
      "----------\n",
      "55 Train Loss: 0.0492, Train Acc: 0.9861\n",
      "55 Val Loss: 0.2519, Val Acc: 0.9116\n",
      "Epoch 56/120\n",
      "----------\n",
      "56 Train Loss: 0.0558, Train Acc: 0.9819\n",
      "56 Val Loss: 0.9629, Val Acc: 0.7764\n",
      "Epoch 57/120\n",
      "----------\n",
      "57 Train Loss: 0.0506, Train Acc: 0.9855\n",
      "57 Val Loss: 0.0689, Val Acc: 0.9838\n",
      "Epoch 58/120\n",
      "----------\n",
      "58 Train Loss: 0.0448, Train Acc: 0.9884\n",
      "58 Val Loss: 0.3362, Val Acc: 0.8946\n",
      "Epoch 59/120\n",
      "----------\n",
      "59 Train Loss: 0.0409, Train Acc: 0.9870\n",
      "59 Val Loss: 0.4140, Val Acc: 0.8776\n",
      "Epoch 60/120\n",
      "----------\n",
      "60 Train Loss: 0.0495, Train Acc: 0.9855\n",
      "60 Val Loss: 0.1497, Val Acc: 0.9575\n",
      "Epoch 61/120\n",
      "----------\n",
      "61 Train Loss: 0.0249, Train Acc: 0.9955\n",
      "61 Val Loss: 0.0405, Val Acc: 0.9898\n",
      "Epoch 62/120\n",
      "----------\n",
      "62 Train Loss: 0.0606, Train Acc: 0.9813\n",
      "62 Val Loss: 2.4660, Val Acc: 0.5918\n",
      "Epoch 63/120\n",
      "----------\n",
      "63 Train Loss: 0.1090, Train Acc: 0.9636\n",
      "63 Val Loss: 0.4239, Val Acc: 0.8537\n",
      "Epoch 64/120\n",
      "----------\n",
      "64 Train Loss: 0.0603, Train Acc: 0.9807\n",
      "64 Val Loss: 0.3684, Val Acc: 0.8844\n",
      "Epoch 65/120\n",
      "----------\n",
      "65 Train Loss: 0.0461, Train Acc: 0.9875\n",
      "65 Val Loss: 0.0929, Val Acc: 0.9651\n",
      "Epoch 66/120\n",
      "----------\n",
      "66 Train Loss: 0.0316, Train Acc: 0.9918\n",
      "66 Val Loss: 0.0310, Val Acc: 0.9932\n",
      "Epoch 67/120\n",
      "----------\n",
      "67 Train Loss: 0.0330, Train Acc: 0.9911\n",
      "67 Val Loss: 0.1074, Val Acc: 0.9643\n",
      "Epoch 68/120\n",
      "----------\n",
      "68 Train Loss: 0.0488, Train Acc: 0.9855\n",
      "68 Val Loss: 0.0178, Val Acc: 0.9940\n",
      "Epoch 69/120\n",
      "----------\n",
      "69 Train Loss: 0.0328, Train Acc: 0.9893\n",
      "69 Val Loss: 0.0661, Val Acc: 0.9821\n",
      "Epoch 70/120\n",
      "----------\n",
      "70 Train Loss: 0.0475, Train Acc: 0.9835\n",
      "70 Val Loss: 0.0398, Val Acc: 0.9872\n",
      "Epoch 71/120\n",
      "----------\n",
      "71 Train Loss: 0.0369, Train Acc: 0.9900\n",
      "71 Val Loss: 0.0387, Val Acc: 0.9855\n",
      "Epoch 72/120\n",
      "----------\n",
      "72 Train Loss: 0.0441, Train Acc: 0.9869\n",
      "72 Val Loss: 0.8828, Val Acc: 0.8010\n",
      "Epoch 73/120\n",
      "----------\n",
      "73 Train Loss: 0.0489, Train Acc: 0.9849\n",
      "73 Val Loss: 0.3573, Val Acc: 0.8912\n",
      "Epoch 74/120\n",
      "----------\n",
      "74 Train Loss: 0.0441, Train Acc: 0.9867\n",
      "74 Val Loss: 0.0842, Val Acc: 0.9711\n",
      "Epoch 75/120\n",
      "----------\n",
      "75 Train Loss: 0.0323, Train Acc: 0.9908\n",
      "75 Val Loss: 0.1727, Val Acc: 0.9439\n",
      "Epoch 76/120\n",
      "----------\n",
      "76 Train Loss: 0.0445, Train Acc: 0.9840\n",
      "76 Val Loss: 1.2097, Val Acc: 0.7738\n",
      "Epoch 77/120\n",
      "----------\n",
      "77 Train Loss: 0.0418, Train Acc: 0.9879\n",
      "77 Val Loss: 1.8739, Val Acc: 0.6794\n",
      "Epoch 78/120\n",
      "----------\n",
      "78 Train Loss: 0.0992, Train Acc: 0.9677\n",
      "78 Val Loss: 0.3401, Val Acc: 0.8810\n",
      "Epoch 79/120\n",
      "----------\n",
      "79 Train Loss: 0.0552, Train Acc: 0.9845\n",
      "79 Val Loss: 0.0274, Val Acc: 0.9940\n",
      "Epoch 80/120\n",
      "----------\n",
      "80 Train Loss: 0.0230, Train Acc: 0.9946\n",
      "80 Val Loss: 0.0479, Val Acc: 0.9813\n",
      "Epoch 81/120\n",
      "----------\n",
      "81 Train Loss: 0.0307, Train Acc: 0.9912\n",
      "81 Val Loss: 0.0163, Val Acc: 0.9957\n",
      "Epoch 82/120\n",
      "----------\n",
      "82 Train Loss: 0.0259, Train Acc: 0.9923\n",
      "82 Val Loss: 0.0826, Val Acc: 0.9753\n",
      "Epoch 83/120\n",
      "----------\n",
      "83 Train Loss: 0.0218, Train Acc: 0.9932\n",
      "83 Val Loss: 0.0173, Val Acc: 0.9974\n",
      "Epoch 84/120\n",
      "----------\n",
      "84 Train Loss: 0.0265, Train Acc: 0.9915\n",
      "84 Val Loss: 0.0488, Val Acc: 0.9813\n",
      "Epoch 85/120\n",
      "----------\n",
      "85 Train Loss: 0.0363, Train Acc: 0.9875\n",
      "85 Val Loss: 0.7150, Val Acc: 0.8469\n",
      "Epoch 86/120\n",
      "----------\n",
      "86 Train Loss: 0.0493, Train Acc: 0.9825\n",
      "86 Val Loss: 0.0722, Val Acc: 0.9762\n",
      "Epoch 87/120\n",
      "----------\n",
      "87 Train Loss: 0.0402, Train Acc: 0.9876\n",
      "87 Val Loss: 0.0193, Val Acc: 0.9966\n",
      "Epoch 88/120\n",
      "----------\n",
      "88 Train Loss: 0.0344, Train Acc: 0.9894\n",
      "88 Val Loss: 0.0912, Val Acc: 0.9694\n",
      "Epoch 89/120\n",
      "----------\n",
      "89 Train Loss: 0.0337, Train Acc: 0.9903\n",
      "89 Val Loss: 0.2218, Val Acc: 0.9354\n",
      "Epoch 90/120\n",
      "----------\n",
      "90 Train Loss: 0.0393, Train Acc: 0.9870\n",
      "90 Val Loss: 0.4149, Val Acc: 0.8750\n",
      "Epoch 91/120\n",
      "----------\n",
      "91 Train Loss: 0.0512, Train Acc: 0.9837\n",
      "91 Val Loss: 0.0623, Val Acc: 0.9813\n",
      "Epoch 92/120\n",
      "----------\n",
      "92 Train Loss: 0.0311, Train Acc: 0.9905\n",
      "92 Val Loss: 0.0207, Val Acc: 0.9932\n",
      "Epoch 93/120\n",
      "----------\n",
      "93 Train Loss: 0.0208, Train Acc: 0.9932\n",
      "93 Val Loss: 0.2015, Val Acc: 0.9405\n",
      "Epoch 94/120\n",
      "----------\n",
      "94 Train Loss: 0.0193, Train Acc: 0.9946\n",
      "94 Val Loss: 0.0079, Val Acc: 0.9983\n",
      "Epoch 95/120\n",
      "----------\n",
      "95 Train Loss: 0.0181, Train Acc: 0.9953\n",
      "95 Val Loss: 0.1272, Val Acc: 0.9668\n",
      "Epoch 96/120\n",
      "----------\n",
      "96 Train Loss: 0.0233, Train Acc: 0.9929\n",
      "96 Val Loss: 0.0170, Val Acc: 0.9957\n",
      "Epoch 97/120\n",
      "----------\n",
      "97 Train Loss: 0.0381, Train Acc: 0.9870\n",
      "97 Val Loss: 0.1554, Val Acc: 0.9490\n",
      "Epoch 98/120\n",
      "----------\n",
      "98 Train Loss: 0.0328, Train Acc: 0.9891\n",
      "98 Val Loss: 0.2217, Val Acc: 0.9218\n",
      "Epoch 99/120\n",
      "----------\n",
      "99 Train Loss: 0.0497, Train Acc: 0.9848\n",
      "99 Val Loss: 0.3241, Val Acc: 0.8980\n",
      "Epoch 100/120\n",
      "----------\n",
      "100 Train Loss: 0.0358, Train Acc: 0.9900\n",
      "100 Val Loss: 0.0713, Val Acc: 0.9719\n",
      "Epoch 101/120\n",
      "----------\n",
      "101 Train Loss: 0.0270, Train Acc: 0.9914\n",
      "101 Val Loss: 0.0197, Val Acc: 0.9940\n",
      "Epoch 102/120\n",
      "----------\n",
      "102 Train Loss: 0.0302, Train Acc: 0.9900\n",
      "102 Val Loss: 0.2077, Val Acc: 0.9354\n",
      "Epoch 103/120\n",
      "----------\n",
      "103 Train Loss: 0.0241, Train Acc: 0.9931\n",
      "103 Val Loss: 0.0341, Val Acc: 0.9889\n",
      "Epoch 104/120\n",
      "----------\n",
      "104 Train Loss: 0.0120, Train Acc: 0.9964\n",
      "104 Val Loss: 0.0224, Val Acc: 0.9932\n",
      "Epoch 105/120\n",
      "----------\n",
      "105 Train Loss: 0.0363, Train Acc: 0.9888\n",
      "105 Val Loss: 0.1062, Val Acc: 0.9583\n",
      "Epoch 106/120\n",
      "----------\n",
      "106 Train Loss: 0.0603, Train Acc: 0.9819\n",
      "106 Val Loss: 0.5983, Val Acc: 0.8673\n",
      "Epoch 107/120\n",
      "----------\n",
      "107 Train Loss: 0.0487, Train Acc: 0.9834\n",
      "107 Val Loss: 0.4091, Val Acc: 0.8767\n",
      "Epoch 108/120\n",
      "----------\n",
      "108 Train Loss: 0.0235, Train Acc: 0.9928\n",
      "108 Val Loss: 0.0095, Val Acc: 0.9974\n",
      "Epoch 109/120\n",
      "----------\n",
      "109 Train Loss: 0.0095, Train Acc: 0.9976\n",
      "109 Val Loss: 0.1280, Val Acc: 0.9626\n",
      "Epoch 110/120\n",
      "----------\n",
      "110 Train Loss: 0.0164, Train Acc: 0.9944\n",
      "110 Val Loss: 0.0083, Val Acc: 1.0000\n",
      "Epoch 111/120\n",
      "----------\n",
      "111 Train Loss: 0.0132, Train Acc: 0.9967\n",
      "111 Val Loss: 0.0236, Val Acc: 0.9932\n",
      "Epoch 112/120\n",
      "----------\n",
      "112 Train Loss: 0.0176, Train Acc: 0.9938\n",
      "112 Val Loss: 0.2224, Val Acc: 0.9345\n",
      "Epoch 113/120\n",
      "----------\n",
      "113 Train Loss: 0.0255, Train Acc: 0.9920\n",
      "113 Val Loss: 0.0619, Val Acc: 0.9770\n",
      "Epoch 114/120\n",
      "----------\n",
      "114 Train Loss: 0.0293, Train Acc: 0.9906\n",
      "114 Val Loss: 0.1412, Val Acc: 0.9524\n",
      "Epoch 115/120\n",
      "----------\n",
      "115 Train Loss: 0.0237, Train Acc: 0.9941\n",
      "115 Val Loss: 0.0157, Val Acc: 0.9949\n",
      "Epoch 116/120\n",
      "----------\n",
      "116 Train Loss: 0.0176, Train Acc: 0.9947\n",
      "116 Val Loss: 0.0368, Val Acc: 0.9889\n",
      "Epoch 117/120\n",
      "----------\n",
      "117 Train Loss: 0.0311, Train Acc: 0.9894\n",
      "117 Val Loss: 0.1643, Val Acc: 0.9456\n",
      "Epoch 118/120\n",
      "----------\n",
      "118 Train Loss: 0.0459, Train Acc: 0.9846\n",
      "118 Val Loss: 0.1399, Val Acc: 0.9575\n",
      "Epoch 119/120\n",
      "----------\n",
      "119 Train Loss: 0.0329, Train Acc: 0.9896\n",
      "119 Val Loss: 0.0356, Val Acc: 0.9872\n",
      "Training complete in 54m 10s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "model = MyCnnModel()\n",
    "\n",
    "#optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model= train_model(model, train_loader, 0.85, criterion, optimizer,num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1q2O4aOMCmoM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCnnModel(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (lstm1): LSTM(64, 64, batch_first=True)\n",
       "  (lstm2): LSTM(64, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=3840, out_features=32, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (fc2): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the best model weights to a file\n",
    "#torch.save(model.state_dict(),\"speechemotion\")\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = MyCnnModel()\n",
    "\n",
    "#model.load_state_dict(torch.load('speechemotion'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "La3yN8aGhJXy"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "output= model(dev_data_X)\n",
    "pre_lab = torch.argmax(output,dim=1)\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "pre_lab = pre_lab.tolist()\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define label dictionary for converting labels to {valence, activation} format\n",
    "\n",
    "label_dict = {0: {\"valence\": 1, \"activation\": 1},\n",
    "              1: {\"valence\": 1, \"activation\": 0},\n",
    "              2: {\"valence\": 0, \"activation\": 0},\n",
    "              3: {\"valence\": 0, \"activation\": 1}}\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "\n",
    "# Write predicted labels to a JSON file with line numbers and {valence, activation} format\n",
    "with open(\"predicted_for_dev_0.85.json\", \"w\") as file:\n",
    "    data = {}\n",
    "    for i, label in enumerate(pre_lab):\n",
    "        data[str(i)] = label_dict[label]\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "test_file = 'test.json'\n",
    "with open(test_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "df=df.T\n",
    "features = df[\"features\"].values\n",
    "lengths = [len(seq) for seq in features]\n",
    "desired_seq_length = 500\n",
    "padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "# loop through each sequence in x\n",
    "for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "      if len(seq) > desired_seq_length:\n",
    "          padded_x[i] = seq[:desired_seq_length]\n",
    "      else:\n",
    "          padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "test_data_X = torch.from_numpy(padded_x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "output= model(test_data_X)\n",
    "pred_lab = torch.argmax(output,dim=1)\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "pred_lab = pred_lab.tolist()\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define label dictionary for converting labels to {valence, activation} format\n",
    "\n",
    "label_dict = {0: {\"valence\": 1, \"activation\": 1},\n",
    "              1: {\"valence\": 1, \"activation\": 0},\n",
    "              2: {\"valence\": 0, \"activation\": 0},\n",
    "              3: {\"valence\": 0, \"activation\": 1}}\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "\n",
    "# Write predicted labels to a JSON file with line numbers and {valence, activation} format\n",
    "with open(\"prediction_test_0.85.json\", \"w\") as file:\n",
    "    data = {}\n",
    "    for i, label in enumerate(pred_lab):\n",
    "        data[str(i)] = label_dict[label]\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
