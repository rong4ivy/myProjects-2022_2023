{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d795f4",
   "metadata": {},
   "source": [
    "# Exercise 7: Explainable Methods\n",
    "\n",
    "In this exercise, you will explore some of the explainable methods that you have learned in the lecture using concrete models and examples from two different modalities: text and images. The focus of this exercise is on the analysis of the explanations, models, and input data. Therefore, you do not need to implement much but instead change hyperparameters and input variables to explore the capabilities and limits of the techniques.\n",
    "\n",
    "We will use the XAI implementations of [Pytorch's captum library](https://captum.ai/api/) which contains several methods that you have seen in the lecture. This is not the only source of good XAI implementations but as part of the Pytorch universe, it is compatible with other Pytorch libraries that we will use (torch, torchtext, torchvision), and will probably be maintained and extended with more techniques in the future. Check out [this summary](https://captum.ai/docs/attribution_algorithms) if you want to see which explanation methods are implemented in this framework.\n",
    "\n",
    "**Note I:** If you have the problem of running out of memory, try restarting the kernel between Task 1 and 2. If the problem remains, reduce hyperparameters like n_steps or n_samples but be aware that your results and findings can be affected by it. If you do this, please state it in your answers.\n",
    "\n",
    "**Note II:** The exercise is designed such that all steps are executable on CPU in reasonable time. Using a GPU would speed everything up, so if you prefer, you can of course adapt the code to run it on GPU. If you do this, please indicate this in your answers. If you do not state anything, we will assume that you run everything on CPU.\n",
    "\n",
    "\n",
    "#### Your tasks in this exercise:\n",
    "* Analyze two neural networks using different explainers (i.e. explanation methods)\n",
    "* Test how the explanations differ depending on settings and input\n",
    "* Relate your observations to the contents of the lecture and analyze them critically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9122449",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before you start, make sure that you have created a virtual environment with Python >= 3.8, and have installed the dependencies given below. \n",
    "\n",
    "#### Requirements:\n",
    "```\n",
    "numpy\n",
    "spacy\n",
    "torch\n",
    "torchvision\n",
    "matplotlib\n",
    "captum\n",
    "```\n",
    "(The versions of these libraries might depend on your version of Python. As a point of reference, the code has been tested with the following versions: Python==3.10.10, numpy==1.23.4, spacy==3.5.3, torch==2.0.0, torchvision==0.15.1, matplotlib==3.6.2, captum==0.6.0)\n",
    "\n",
    "Once you have created the environment, run this command on the command line:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "This will install the language model that we need for preprocessing the text input.\n",
    "\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import the explanation methods we are going to use in this exercise\n",
    "from captum.attr import (LayerIntegratedGradients,\n",
    "                        LayerDeepLift,\n",
    "                        LayerActivation,\n",
    "                        Lime,\n",
    "                        KernelShap,\n",
    "                        IntegratedGradients)\n",
    "# Import some helper functions for the explainers\n",
    "from captum.attr import (TokenReferenceBase,  # to create reference inputs\n",
    "                         NoiseTunnel,  # to run explainers on noisy copies of the input\n",
    "                         LayerAttribution,  # to scale attribution maps to input size\n",
    "                         configure_interpretable_embedding_layer,  # to make embeddings interpretable\n",
    "                         remove_interpretable_embedding_layer,  # to revert embeddings to non-interpretable default\n",
    "                         visualization)  # to visualize explanations\n",
    "from captum._utils.models.linear_model.model import SkLearnLasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e6eb2",
   "metadata": {},
   "source": [
    "## Task 1: Analyzing an NLP model\n",
    "\n",
    "In this task, we will look at a classical NLP classification task that is often used in XAI: Sentiment analysis. Our model has been trained to predict whether a text displays a positive sentiment or a negative one, making it a binary decision. The model itself is a CNN that takes [GloVe word embeddings](https://nlp.stanford.edu/projects/glove/) as input and outputs a scalar prediction value. If that value is < 0.5, the predicted sentiment is *negative* (0), otherwise *positive* (1). \n",
    "\n",
    "The code and models in this task are taken from [this tutorial](https://captum.ai/tutorials/IMDB_TorchText_Interpret). Before you run the code, download the model [imdb-model-cnn-large.pt](https://github.com/pytorch/captum/blob/master/tutorials/models/imdb-model-cnn-large.pt). The vocabulary and matching GLoVe indices are already prepared in vocab.json.\n",
    "\n",
    "Now, run the following five code cells and answer the questions at the end of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa69cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment classification model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1) # [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] #[batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1)) # [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)\n",
    "    \n",
    "# Load the model\n",
    "model = torch.load('imdb-model-cnn-large.pt')\n",
    "model.eval()\n",
    "    \n",
    "def forward_with_sigmoid(inp):\n",
    "    return torch.sigmoid(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, vocab_path):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab.update(json.load(f))\n",
    "            \n",
    "        self.pad_token = '<pad>'\n",
    "        self.pad_ind = self.vocab[self.pad_token]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    # This preprocesses the input and returns the tokenized sentence (text) as well as the encoded one (input_indices)\n",
    "    def prepare_sentence(self, sentence, min_len=7):\n",
    "        text = [tok.text for tok in self.nlp.tokenizer(sentence.lower())]\n",
    "        if len(text) < min_len:\n",
    "            text += [self.pad_token] * (min_len - len(text))\n",
    "        indexed = [self.vocab[t] for t in text]\n",
    "\n",
    "        input_indices = torch.tensor(indexed)\n",
    "        input_indices = input_indices.unsqueeze(0)\n",
    "\n",
    "        return text, input_indices\n",
    "\n",
    "preprocess = Preprocessing(vocab_path='vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: 'neg', 1: 'pos'}\n",
    "\n",
    "# Once we get the attributions (\"explanations\") for a sentence from an explainer, we will store it in a specific \n",
    "# DataRecord object that will help us to visualize the explanation\n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    return visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            classes[pred_ind],\n",
    "                            classes[label],\n",
    "                            classes[1],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5a6a5",
   "metadata": {},
   "source": [
    "We are going to test five different explainers: [Layer Activation](https://captum.ai/api/layer.html#layer-activation), [Layer Integrated Gradients](https://captum.ai/api/layer.html#layer-integrated-gradients), [Layer DeepLIFT](https://captum.ai/api/layer.html#layer-deeplift), [LIME](https://captum.ai/api/lime.html), and [KernelShap](https://captum.ai/api/kernel_shap.html). \n",
    "\n",
    "**Layer Integrated Gradients** and **Layer DeepLIFT** are variants of the methods *Integrated Gradients* and *DeepLIFT* which have been described in the lecture. The *Layer* modifier just indicates that instead of computing the influence of the *input* on the output prediction, we compute it for the nodes of a specific *layer*. This makes sense if our network first performs a feature extraction on some raw input, and we care more about the meaning of each feature. In our case, we will use the layer variants for the *embedding* layer because this allows us to skip an unnecessary step: Since the transformation process from input to embeddings is fairly linear (one token corresponds to one embedding), we do not need to explain this step, and can directly start with the embeddings as input. Also, it is very simple to project the explanations that we get for our embeddings back to our input tokens (since it is a 1:1 projection).\n",
    "\n",
    "**Layer Activation** is a very simple method which we have not covered in the lecture. It simply outputs the activation values of a specific layer as computed during the forward pass of the input sample. We can project the nodes back to the input to get a score of how much the input feature led to activation in the layer, which, depending on the network, can give us an estimate for the activation in the overall model. Again, we will use the *embedding* layer for this method.\n",
    "\n",
    "**LIME** and **KernelShap** are used as described in the lecture. They are model-agnostic and work on input perturbation, thus, we are not starting at a layer inside the model but use the whole model for generating the explanations. However, specific to the captum library and its internal processing, we need to wrap the embeddings layer into an interpretable embeddings layer (and later remove this wrapping again). You do not need to worry about this unless, for some reason, there is an error raised within the explain_with_Lime or explain_with_KernelShap methods and the layer could not be removed again. If this happens, it is easiest to simply reload the model with `model = torch.load('imdb-model-cnn-large.pt')` to reset the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For several explainers, we need a reference input as a baseline. Usually, this is just a zero or random vector, \n",
    "# but we can just create a reference input based on the vector corresponding to the padding token\n",
    "token_reference = TokenReferenceBase(reference_token_idx=preprocess.pad_ind)\n",
    "\n",
    "\n",
    "# We are going to apply several explainers to the model to examine the similarities and differences between them\n",
    "\n",
    "# 1. LayerIntegratedGradients\n",
    "def explain_with_LayerIntegratedGradients(sentences, min_len=7):\n",
    "    vis_data_records = []\n",
    "    lig = LayerIntegratedGradients(model, model.embedding)\n",
    "\n",
    "    for sentence, label in sentences:\n",
    "        model.zero_grad()\n",
    "        text, input_indices = preprocess.prepare_sentence(sentence, min_len)\n",
    "\n",
    "        # Predict\n",
    "        pred = forward_with_sigmoid(input_indices).item()\n",
    "        pred_ind = round(pred)\n",
    "\n",
    "        # Generate reference indices for each sample\n",
    "        reference_indices = token_reference.generate_reference(min_len, device='cpu').unsqueeze(0)\n",
    "\n",
    "        # Compute attributions\n",
    "        attributions = lig.attribute(input_indices, reference_indices, n_steps=1000)\n",
    "        \n",
    "        vis_data_records.append(add_attributions_to_visualizer(attributions, text, pred, pred_ind, label))\n",
    "        \n",
    "    print('Visualize attributions based on Layer Integrated Gradients:')\n",
    "    _ = visualization.visualize_text(vis_data_records)\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "# 2. LayerDeepLift\n",
    "def explain_with_LayerDeepLift(sentences, min_len=7):\n",
    "    vis_data_records = []\n",
    "    ldl = LayerDeepLift(model, model.embedding)\n",
    "\n",
    "    for sentence, label in sentences:\n",
    "        model.zero_grad()\n",
    "        text, input_indices = preprocess.prepare_sentence(sentence, min_len)\n",
    "\n",
    "        # Predict\n",
    "        pred = forward_with_sigmoid(input_indices).item()\n",
    "        pred_ind = round(pred)\n",
    "\n",
    "        # Generate reference indices for each sample\n",
    "        reference_indices = token_reference.generate_reference(min_len, device='cpu').unsqueeze(0)\n",
    "\n",
    "        # Compute attributions\n",
    "        attributions = ldl.attribute(input_indices, reference_indices)\n",
    "        \n",
    "        vis_data_records.append(add_attributions_to_visualizer(attributions, text, pred, pred_ind, label))\n",
    "        \n",
    "    print('Visualize attributions based on Layer Deep Lift:')\n",
    "    _ = visualization.visualize_text(vis_data_records)\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "# 3. LayerActivation\n",
    "def explain_with_LayerActivation(sentences, min_len=7):\n",
    "    vis_data_records = []\n",
    "    la = LayerActivation(model, model.embedding)\n",
    "\n",
    "    for sentence, label in sentences:\n",
    "        model.zero_grad()\n",
    "        text, input_indices = preprocess.prepare_sentence(sentence, min_len)\n",
    "\n",
    "        # Predict\n",
    "        pred = forward_with_sigmoid(input_indices).item()\n",
    "        pred_ind = round(pred)\n",
    "\n",
    "        # Compute attributions\n",
    "        attributions = la.attribute(input_indices)\n",
    "        \n",
    "        vis_data_records.append(add_attributions_to_visualizer(attributions, text, pred, pred_ind, label))\n",
    "        \n",
    "    print('Visualize attributions based on Layer Activation:')\n",
    "    _ = visualization.visualize_text(vis_data_records)\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "# 4. Lime\n",
    "def explain_with_Lime(sentences, min_len=7):\n",
    "    vis_data_records = []\n",
    "    interpretable_emb = configure_interpretable_embedding_layer(model, 'embedding')\n",
    "    lime = Lime(model, interpretable_model=SkLearnLasso(alpha=0.1))\n",
    "\n",
    "    for sentence, label in sentences:\n",
    "        model.zero_grad()\n",
    "        text, input_indices = preprocess.prepare_sentence(sentence, min_len)\n",
    "        input_emb = interpretable_emb.indices_to_embeddings(input_indices)\n",
    "\n",
    "        # Predict\n",
    "        pred = forward_with_sigmoid(input_emb).item()\n",
    "        pred_ind = round(pred)\n",
    "\n",
    "        # Generate reference embeddings for each sample\n",
    "        reference_indices = token_reference.generate_reference(min_len, device='cpu').unsqueeze(0)\n",
    "        reference_emb = interpretable_emb.indices_to_embeddings(reference_indices)\n",
    "\n",
    "        # Compute attributions\n",
    "        attributions = lime.attribute(input_emb, baselines=reference_emb, n_samples=50)\n",
    "        \n",
    "        vis_data_records.append(add_attributions_to_visualizer(attributions, text, pred, pred_ind, label))\n",
    "        \n",
    "    remove_interpretable_embedding_layer(model, interpretable_emb)\n",
    "        \n",
    "    print('Visualize attributions based on Lime:')\n",
    "    _ = visualization.visualize_text(vis_data_records)\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "# 5. KernelShap\n",
    "def explain_with_KernelShap(sentences, min_len=7):\n",
    "    vis_data_records = []\n",
    "    interpretable_emb = configure_interpretable_embedding_layer(model, 'embedding')\n",
    "    ks = KernelShap(model)\n",
    "\n",
    "    for sentence, label in sentences:\n",
    "        model.zero_grad()\n",
    "        text, input_indices = preprocess.prepare_sentence(sentence, min_len)\n",
    "        input_emb = interpretable_emb.indices_to_embeddings(input_indices)\n",
    "\n",
    "        # Predict\n",
    "        pred = forward_with_sigmoid(input_emb).item()\n",
    "        pred_ind = round(pred)\n",
    "\n",
    "        # Generate reference embeddings for each sample\n",
    "        reference_indices = token_reference.generate_reference(min_len, device='cpu').unsqueeze(0)\n",
    "        reference_emb = interpretable_emb.indices_to_embeddings(reference_indices)\n",
    "\n",
    "        # Compute attributions\n",
    "        attributions = ks.attribute(input_emb, baselines=reference_emb, n_samples=100)\n",
    "        \n",
    "        vis_data_records.append(add_attributions_to_visualizer(attributions, text, pred, pred_ind, label))\n",
    "        \n",
    "    remove_interpretable_embedding_layer(model, interpretable_emb)\n",
    "        \n",
    "    print('Visualize attributions based on KernelShap:')\n",
    "    _ = visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320c4b4",
   "metadata": {},
   "source": [
    "Now, we got everything ready. Let's try it with some test sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce234c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = [ # sentence, label (1: positive, 0: negative)\n",
    "    ('Best film ever', 1),\n",
    "    ('It was a horrible movie: never again.', 0),\n",
    "    ('I\\'ve never watched something as bad...', 0),\n",
    "    ('That is a terrible movie.', 0),\n",
    "    ('This was awfully beautiful.', 1),\n",
    "    ('I did not feel the urge to leave the cinema immediately.', 1),\n",
    "    ('This was a lousy performance!', 0),\n",
    "    ('And they call this good???', 0),\n",
    "    ('I had tears of joy in my eyes', 1),\n",
    "    ('I did not expect such a well-written play!', 1),\n",
    "    ('This was too good to be true', 1),\n",
    "    ('This was amazing', 1)\n",
    "]\n",
    "\n",
    "# We will pad all sentences to the length of the longest sentence (including punctuation)\n",
    "# If you want to use longer sentences, you might need to increase this number\n",
    "min_len = 12\n",
    "\n",
    "explain_with_LayerIntegratedGradients(sentences, min_len)\n",
    "explain_with_LayerDeepLift(sentences, min_len)\n",
    "explain_with_LayerActivation(sentences, min_len)\n",
    "explain_with_Lime(sentences, min_len)\n",
    "explain_with_KernelShap(sentences, min_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f9bd6",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "#### 1) Analysis of explanations\n",
    "Analyze the explanations as given by each explainer for the given test sentences. Do they correspond to your expectations, i.e. would you base your decision for a positive or negative label on the same words as the model (according to the explanations) does? Can you understand why the model has different prediction scores (in brackets after the predicted label) and why it might have predicted the wrong label?\n",
    "\n",
    "\n",
    "#### 2) Comparison of explainers\n",
    "What are the differences and similarities between the explainers, given the word importance graphics and the attribution scores? Can you relate your observations to the basic functionality of each method as described in the lecture?\n",
    "\n",
    "\n",
    "#### 3) Ranking: Explanation quality\n",
    "Which explainer produces in your opinion the best explanations? Create a ranking from (1) most sensible to (5) least sensible. Explain your decision.\n",
    "\n",
    "#### 4) Ranking: Execution time\n",
    "Use a method to measure the execution time of each *explain_with_x* run (e.g. with the python libraries [time](https://docs.python.org/3/library/time.html) or [timeit](https://docs.python.org/3/library/timeit.html)), and rank the explainers from (1) fastest to (5) slowest. To get more expressive results, you can increase the number of sentences or simply iterate over the same sentences a couple of times. Do you see any differences between the execution times? Can you explain them using your knowledge about the basic functionality of each explainer?\n",
    "\n",
    "\n",
    "#### 5) Hyperparameters\n",
    "Some explainers have hyperparameters that can significantly influence the explanations. Adjust the following hyperparameters in a way that they produce the best explanations in as little execution time as possible. Report your final value for each hyperparameter, and briefly describe how they affect the result. \n",
    "\n",
    "*Note:* You do not need to measure the *goodness* or *accuracy* of the explanations in any other way than with your personal perception. Also, you do not need to find the very best setting, just one setting that works well enough. \n",
    "\n",
    "* a) **n_steps**: The method **LayerIntegratedGradients** has a hyperparameter *n_steps* which denotes the number of steps of interpolation between reference and input. It is given as positive integer.\n",
    "* b) **n_samples:** Similarly to n_steps, *n_samples* is a hyperparameter in **LIME** and **KernelShap** to set the number of samples used to train a surrogate model. It is given as positive integer too.\n",
    "* c) **alpha**: **LIME** actually has a large number of hyperparameters. You can e.g. choose the simple surrogate model to approximate the local decision or the way how to perturb input data. We will focus on a single hyperparameter here that is actually not a hyperparameter of LIME itself but of the surrogate model that we are training: [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). Lasso is a linear regression model using L1 regularization to avoid overfitting. The *alpha* hyperparameter is a positive floating point value denoting a scaling factor of the L1 regularization. Higher *alpha* thus corresponds to higher regularization (and less fit to the training data). \n",
    "\n",
    "\n",
    "#### 6) Reranking\n",
    "Repeat the rankings from 3) and 4) on the explainers with the settings you found in 5). Does something change? Why?\n",
    "\n",
    "\n",
    "#### 7) Input modification\n",
    "Pick one explainer and experiment with different input words. Does the prediction of the model and the explanation change when you remove or change the word(s) that is marked as most important by the explainer? What happens if you change or remove the least important word(s)? Can you create examples that will deliberately fool the model into making the wrong prediction? How does the explanation look like then?\n",
    "\n",
    "\n",
    "#### 8) Discussion\n",
    "Do you think that the attributions as produced by the explanation methods and visualized as heatmaps are actually *explaining the model*? What do they show, and is this output helpful or desirable for understanding the prediction? Could you imagine a better way of explaining an NLP model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6779b",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "#### 1) \n",
    "\n",
    "\n",
    "#### 2)\n",
    "\n",
    "\n",
    "#### 3)\n",
    "\n",
    "\n",
    "#### 4)\n",
    "\n",
    "\n",
    "#### 5) \n",
    "\n",
    "\n",
    "#### 6)\n",
    "\n",
    "\n",
    "#### 7)\n",
    "\n",
    "\n",
    "#### 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89565af",
   "metadata": {},
   "source": [
    "## Task 2: Analyzing a Computer Vision model\n",
    "\n",
    "The most prominent modality in the research of XAI is image. Most of the explanation methods are first developed with computer vision applications in mind, and for humans it is easier to understand something that is visualized. \n",
    "\n",
    "We are going to use images coming from [this repository](https://github.com/adebayoj/sanity_checks_saliency/tree/master/data/demo_images). The overall code is following [this tutorial](https://captum.ai/tutorials/Resnet_TorchVision_Interpret).\n",
    "\n",
    "We are going to use a basic image classification model, ResNet18. It has been pretrained on the ImageNet classification task, which consists of 1000 object classes, and each image is predicted as one of the classes according to the main object in the image. The pretrained model is provided by PyTorch, so we can simply download it by running the next lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e1fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet18 model and print its layers\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd7056",
   "metadata": {},
   "source": [
    "Now that we have our model, we need to load our image data and prepare it for the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06effe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images in {data_path}/{image_files}\n",
    "data_path = 'test_images'\n",
    "image_files = ['image1.png', 'image2.png', 'image3.jpeg', 'image4.jpeg', 'image5.jpeg', 'image6.jpeg', 'image7.jpeg']\n",
    "\n",
    "# We will directly transform the images into the format as expected by the model\n",
    "# We skip the normalization at this point to make it easier to visualize the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),    # transform into image with RGB values on 3 channels\n",
    "    transforms.Resize(256),     # resize image into (3, 256, 256)\n",
    "    transforms.CenterCrop(224), # crop the image to the center (3, 224, 224)\n",
    "    transforms.ToTensor(),      # transform into tensor\n",
    "])\n",
    "\n",
    "images = []\n",
    "for filename in image_files:\n",
    "    filepath = f'{data_path}/{filename}'\n",
    "    img = torchvision.io.read_image(filepath)\n",
    "    images.append(transform(img))\n",
    "\n",
    "# For using the images in the model, we will need to normalize them towards the mean and std (on each color channel)\n",
    "# of the whole imagenet training set\n",
    "# We will define the transformation function here but use it later right before the prediction\n",
    "normalize_transform = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# The imagenet classification task has 1000 class labels but they are not stored in the model (only the indices)\n",
    "# To understand the model predictions, we need to load an external class mapping\n",
    "with open('imagenet_class_index.json', 'r') as f:\n",
    "    imagenet_classes = json.load(f)\n",
    "idx2label = {int(k): v[1] for k, v in imagenet_classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588391ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the images\n",
    "f, axarr = plt.subplots(1, 7, figsize=(20, 10))\n",
    "for i in range(len(images)):\n",
    "    axarr[i].imshow(np.moveaxis(images[i].numpy(), 0, -1))\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'image{i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d186eb",
   "metadata": {},
   "source": [
    "Before you continue with the rest of the task, pause a moment and think about how you would label each image with one object name (e.g. tree).\n",
    "\n",
    "----\n",
    "\n",
    "The next cell defines our explain method using variations of Integrated Gradients. You have already met the *layer* variant in Task 1. In this task, we will now compare that variant to the standard one: Does it make a difference whether we apply the technique to the whole network (i.e., [**IntegratedGradients (IG)**](https://captum.ai/api/integrated_gradients.html)) or just a part of it (i.e.  [**LayerIntegratedGradients (LIG)**](https://captum.ai/api/layer.html#layer-integrated-gradients))? Why do we even bother to analyze both when for Task 1, the LIG variant was sufficient? The reason for this is that our modality in Task 2 (image) is fundamentally different from the modality in Task 1 (text), and so are the functionalities of our models. For instance, in this CNN model, our first layer does not perform a linear translation into an encoded representation of our input as it was the case for the embeddings in Task 1, but actually reduces groups of input pixels to some shared meaning. In other words, we perform a feature extraction within the model. In fact, almost all layers in an image classification CNN can be seen as feature extraction, reduction and processing layers, and only the final layers are actually responsible for the classification. Therefore, explanation methods in such models are often applied only to the last layer, expecting that it would tell us most about why certain groups of pixels are more important for the distinction between \"house\" and \"castle\", for example. But are we not simplifying this too much now? Should we not look at the whole network and regard the feature extraction layers as equally relevant for the final decision? For a better understanding, we will try both in this task. \n",
    "\n",
    "We can choose between using IG or LIG via the parameter *layer_specific*. If *layer_specific=True*, we will use LIG, otherwise the normal IG. The exact layer that we are computing the gradients for in the former case can be controlled via *layer*. \n",
    "\n",
    "One option to increase the expressiveness of our explanations is to use a [**NoiseTunnel**](https://captum.ai/api/noise_tunnel.html). This means that we run our attribution operation several (*nt_samples*) times on the same input but each time we add Gaussian noise to the input to get a slightly different version of it. The attribution maps we get after each time are finally combined to one heatmap for the input image. The method *how* we combine them is controlled via *nt_type*: we can either compute the mean of the attribution maps (*smoothgrad*), the mean of the squared attribution maps (*smoothgrad_sq*), or the variance of the attribution mas (*vargrad*). Whether to use a noise tunnel or not is controlled via the *noise_tunnel* parameter.\n",
    "\n",
    "We will also use one parameter *labels* for selecting the label that we want to have the explanations for. This means that we do not have to compute the explanation for the actual predicted class of an image but can use in fact any of the 1000 ImageNet classes. That can be helpful to analyze where the model would focus on to decide for a different, but also possible class. For now, we will not care about this, set *labels=None*, and create the explanations for the actual predicted classes of the model.\n",
    "\n",
    "The explanations will be visualized as row of three images each: the original image, the heatmap, and a fusion of original image and heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4ddc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'), (0.25, '#000000'), (1, '#000000')], \n",
    "                                                 N=256)\n",
    "\n",
    "\n",
    "def explain_images(images, labels=None, layer_specific=False, noise_tunnel=False, layer=model.layer4[-1]):\n",
    "    if layer_specific:\n",
    "        explainer = LayerIntegratedGradients(model, layer)\n",
    "        print('Explain images with Layer Integrated Gradients')\n",
    "    else:\n",
    "        explainer = IntegratedGradients(model)\n",
    "        print('Explain images with Integrated Gradients')\n",
    "        \n",
    "    attribute_params = {'n_steps': 200}\n",
    "        \n",
    "    if noise_tunnel:\n",
    "        explainer = NoiseTunnel(explainer)\n",
    "        attribute_params.update({'nt_samples': 2, 'nt_type': 'smoothgrad'})\n",
    "        print('Use Noise Tunnel to increase expressiveness')\n",
    "\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        x = img.unsqueeze(0)\n",
    "        normalized_x = normalize_transform(x)\n",
    "        \n",
    "        output = F.softmax(model(normalized_x), dim=1).squeeze()\n",
    "        pred_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "        target = labels[i] if labels and labels[i] else pred_label_idx.item()\n",
    "        \n",
    "        target_list = target if isinstance(target, list) else [target]\n",
    "        \n",
    "        for t in target_list:\n",
    "            attributions = explainer.attribute(normalized_x, target=t, **attribute_params)\n",
    "\n",
    "            # We need to scale the attribution to the input shape if the explanation was produced for a layer with\n",
    "            # less nodes than our input image\n",
    "            if attributions.shape[-1] != 224:\n",
    "                attributions = LayerAttribution.interpolate(attributions, (224, 224), 'bicubic')\n",
    "\n",
    "            _ = visualization.visualize_image_attr_multiple(\n",
    "                                     np.transpose(attributions.squeeze(0).detach().numpy(), (1,2,0)),\n",
    "                                     np.transpose(x.squeeze().detach().numpy(), (1,2,0)),\n",
    "                                     methods=[\"original_image\", \"heat_map\", \"blended_heat_map\"],\n",
    "                                     cmap=default_cmap,\n",
    "                                     show_colorbar=True,\n",
    "                                     signs=[\"all\", \"positive\", \"positive\"])\n",
    "\n",
    "            print(f'Predicted label: {idx2label[pred_label_idx.item()]}, score: {pred_score.item()}, '\n",
    "                  f'heat map for label: {idx2label[t]}')\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dffa1f",
   "metadata": {},
   "source": [
    "Let's run Integrated Gradients to explain our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6246d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_images(images, layer_specific=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24589f",
   "metadata": {},
   "source": [
    "And Layer Integrated Gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29101a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_images(images, layer_specific=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b9323",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "##### 1) Classification labels\n",
    "Look at each image and think about how you would classify it with one term (corresponding to the main object in the image). Then compare that with the prediction of the model. Do you agree in each case?\n",
    "\n",
    "##### 2) Explanations to classification labels\n",
    "For each image, check the heatmaps that are visualized as explanation (separately or laid over the image). How well do the heatmaps correspond to what you would have expected? How well to the predictions?\n",
    "\n",
    "##### 3) Noise tunnel\n",
    "Set noise tunnel to true. Select 3 images and run the explainers for them. How does the noise tunnel influence the explanations? Explain why.\n",
    "\n",
    "##### 4) Hyperparameters for LIG and IG\n",
    "Besides the **n_steps** hyperparameter of (layer) integrated gradients that you already encountered in Task 1, the noise tunnel now introduces two additional hyperparameters: **nt_samples** (how many noisy examples to create for one input) and **nt_type** (smoothing method for the attributions). \n",
    "\n",
    "Select one image and try different settings for the three hyperparameters. What do you find out? What seems to be the best setting? Explain why.\n",
    "\n",
    "*Note: Some hyperparameters have a big effect on runtime and memory usage. It can happen that the notebook crashes if the values are too high. Therefore, we do not expect you to try nt_steps above 500 or nt_samples above 10 in this exercise.*\n",
    "\n",
    "##### 5) Explanations for different possible labels\n",
    "Some images contain several objects that could have been predicted by the model instead of the label it actually outputs. Example: image 6 could be the dog or the bucket. For these images, pick a label for each of such objects from the *idx2label* mapping (if there is a label), and input the index of that label to the *label* parameter of *explain_images*. \n",
    "\n",
    "The parameter should be a list in which each position corresponds to one of the seven images, indicating the label(s) that should be used for calculating the attribution. If you give several labels for one image (as nested list), the image will be processed several times. If you want to use the predicted label for one image instead of a manually selected one, input *None*. Example: [None, [0, 12], 13, None, [6, 7, 9], None, 26]. How do the heatmap explanations change? Do they make sense?\n",
    "\n",
    "##### 6) Explanations for nonsensical labels\n",
    "What happens if you input labels that do not make sense for a specific image; e.g. 'orangutan' for image 1? Explain why.\n",
    "\n",
    "##### 7) Discussion\n",
    "Analogous to Task 1: Do you think the heatmaps are actually *explaining the model*? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d416e8",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "#### 1) \n",
    "\n",
    "\n",
    "#### 2)\n",
    "\n",
    "\n",
    "#### 3)\n",
    "\n",
    "\n",
    "#### 4)\n",
    "\n",
    "\n",
    "#### 5) \n",
    "\n",
    "\n",
    "#### 6)\n",
    "\n",
    "\n",
    "#### 7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
